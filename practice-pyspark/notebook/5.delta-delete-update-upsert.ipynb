{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc343b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41436d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_location = 'hdfs://hdfs:9000/hive/warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d795b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee2ce7dc-bfca-4ea3-8f5d-179cf96ed6a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 695ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee2ce7dc-bfca-4ea3-8f5d-179cf96ed6a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/15ms)\n",
      "22/01/21 02:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('spark://master:7077') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48dd4e2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2c0b1e7f5e19:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fef9fde5c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fb463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://hdfs:9000/hive/warehouse/people10m\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -rm -r hdfs://hdfs:9000/hive/warehouse/people10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3124353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.people10m (\n",
    "  id INT,\n",
    "  firstName STRING,\n",
    "  middleName STRING,\n",
    "  lastName STRING,\n",
    "  gender STRING,\n",
    "  birthDate TIMESTAMP,\n",
    "  ssn STRING,\n",
    "  salary INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b7e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "INSERT INTO default.people10m\n",
    "values (1, 'husein', 'bin', 'zolkepli', 'm', '2021-01-01 00:00:01', '1', 100)\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d75cbc22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:============================================>           (40 + 3) / 50]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  1|   husein|       bin|zolkepli|     m|2021-01-01 00:00:01|  1|   100|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from default.people10m')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6193c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fef9e58ddf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.createIfNotExists(spark) \\\n",
    "  .tableName(\"default.people10m\") \\\n",
    "  .addColumn(\"id\", \"INT\") \\\n",
    "  .addColumn(\"firstName\", \"STRING\") \\\n",
    "  .addColumn(\"middleName\", \"STRING\") \\\n",
    "  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "  .addColumn(\"gender\", \"STRING\") \\\n",
    "  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "  .addColumn(\"ssn\", \"STRING\") \\\n",
    "  .addColumn(\"salary\", \"INT\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eff0239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fef9e58d520>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.createOrReplace(spark) \\\n",
    "  .addColumn(\"id\", \"INT\") \\\n",
    "  .addColumn(\"firstName\", \"STRING\") \\\n",
    "  .addColumn(\"middleName\", \"STRING\") \\\n",
    "  .addColumn(\"lastName\", \"STRING\") \\\n",
    "  .addColumn(\"gender\", \"STRING\") \\\n",
    "  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "  .addColumn(\"ssn\", \"STRING\") \\\n",
    "  .addColumn(\"salary\", \"INT\") \\\n",
    "  .location(f\"{warehouse_location}/people10m\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e1d0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  2|    hasan|       bin|zolkepli|     m|2000-01-01 00:00:01|  2|  1000|\n",
      "|  3|    husna|     binti|zolkepli|     f|1996-01-01 00:00:01|  3|   500|\n",
      "|  4|      ayu|     binti|    dzul|     f|1990-01-01 00:00:01|  4|   100|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data = [(2, 'hasan', 'bin', 'zolkepli', 'm', \n",
    "         datetime.strptime('2000-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '2', 1000),\n",
    "        (3, 'husna', 'binti', 'zolkepli', 'f', \n",
    "         datetime.strptime('1996-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '3', 500),\n",
    "        (4, 'ayu', 'binti', 'dzul', 'f', \n",
    "         datetime.strptime('1990-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '4', 100),]\n",
    "  \n",
    "dataframe = spark.createDataFrame(data, df_sql.schema)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66d2a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataframe.write.format('delta').mode('append').saveAsTable(\"default.people10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4951fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  1|   husein|       bin|zolkepli|     m|2021-01-01 00:00:01|  1|   100|\n",
      "|  2|    hasan|       bin|zolkepli|     m|2000-01-01 00:00:01|  2|  1000|\n",
      "|  3|    husna|     binti|zolkepli|     f|1996-01-01 00:00:01|  3|   500|\n",
      "|  4|      ayu|     binti|    dzul|     f|1990-01-01 00:00:01|  4|   100|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from default.people10m')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4d1f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forName(spark, 'default.people10m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9b2c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable.delete(\"birthDate < '1955-01-01'\")\n",
    "deltaTable.delete(col('birthDate') < '1995-01-011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11c680be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  3|    husna|     binti|zolkepli|     f|1996-01-01 00:00:01|  3|   500|\n",
      "|  1|   husein|       bin|zolkepli|     m|2021-01-01 00:00:01|  1|   100|\n",
      "|  2|    hasan|       bin|zolkepli|     m|2000-01-01 00:00:01|  2|  1000|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from default.people10m')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "840e2fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable.update(\n",
    "  condition = \"gender = 'f'\",\n",
    "  set = { \"gender\": \"'Female'\" }\n",
    ")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('gender') == 'm',\n",
    "  set = { 'gender': lit('Male') }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6e5bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  3|    husna|     binti|zolkepli|Female|1996-01-01 00:00:01|  3|   500|\n",
      "|  1|   husein|       bin|zolkepli|  Male|2021-01-01 00:00:01|  1|   100|\n",
      "|  2|    hasan|       bin|zolkepli|  Male|2000-01-01 00:00:01|  2|  1000|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from default.people10m')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "328fcba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  2|    hasan|       bin|zolkepli|     m|2000-01-01 00:00:01|  2|  1000|\n",
      "|  3|    husna|     binti|zolkepli|     f|1996-01-01 00:00:01|  3|   500|\n",
      "|  4|      ayu|     binti|    dzul|     f|1990-01-01 00:00:01|  4|   100|\n",
      "|  5|   ayu-v2|     binti|    dzul|     f|1990-01-01 00:00:01|  5|   100|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [(2, 'hasan', 'bin', 'zolkepli', 'm', \n",
    "         datetime.strptime('2000-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '2', 1000),\n",
    "        (3, 'husna', 'binti', 'zolkepli', 'f', \n",
    "         datetime.strptime('1996-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '3', 500),\n",
    "        (4, 'ayu', 'binti', 'dzul', 'f', \n",
    "         datetime.strptime('1990-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '4', 100),\n",
    "        (5, 'ayu-v2', 'binti', 'dzul', 'f', \n",
    "         datetime.strptime('1990-01-01 00:00:01', '%Y-%m-%d %H:%M:%S'), '5', 100)]\n",
    "  \n",
    "dataframe = spark.createDataFrame(new_data, df_sql.schema)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "846ba73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataframe.write.format('delta').mode('append').saveAsTable(\"default.people10m_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ec715d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTablePeople = DeltaTable.forName(spark, 'default.people10m')\n",
    "deltaTablePeopleUpdates = DeltaTable.forName(spark, 'default.people10m_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b63ea30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, firstName: string, middleName: string, lastName: string, gender: string, birthDate: timestamp, ssn: string, salary: int]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUpdates = deltaTablePeopleUpdates.toDF()\n",
    "dfUpdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a30eec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 03:59:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTablePeople.alias('people') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'people.id = updates.id'\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"id\": \"updates.id\",\n",
    "      \"firstName\": \"updates.firstName\",\n",
    "      \"middleName\": \"updates.middleName\",\n",
    "      \"lastName\": \"updates.lastName\",\n",
    "      \"gender\": \"updates.gender\",\n",
    "      \"birthDate\": \"updates.birthDate\",\n",
    "      \"ssn\": \"updates.ssn\",\n",
    "      \"salary\": \"updates.salary\"\n",
    "    }\n",
    "  ) \\\n",
    "  .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"id\": \"updates.id\",\n",
    "      \"firstName\": \"updates.firstName\",\n",
    "      \"middleName\": \"updates.middleName\",\n",
    "      \"lastName\": \"updates.lastName\",\n",
    "      \"gender\": \"updates.gender\",\n",
    "      \"birthDate\": \"updates.birthDate\",\n",
    "      \"ssn\": \"updates.ssn\",\n",
    "      \"salary\": \"updates.salary\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18aad4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "| id|firstName|middleName|lastName|gender|          birthDate|ssn|salary|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "|  1|   husein|       bin|zolkepli|  Male|2021-01-01 00:00:01|  1|   100|\n",
      "|  3|    husna|     binti|zolkepli|     f|1996-01-01 00:00:01|  3|   500|\n",
      "|  2|    hasan|       bin|zolkepli|     m|2000-01-01 00:00:01|  2|  1000|\n",
      "|  5|   ayu-v2|     binti|    dzul|     f|1990-01-01 00:00:01|  5|   100|\n",
      "|  4|      ayu|     binti|    dzul|     f|1990-01-01 00:00:01|  4|   100|\n",
      "+---+---------+----------+--------+------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from default.people10m')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467d5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
