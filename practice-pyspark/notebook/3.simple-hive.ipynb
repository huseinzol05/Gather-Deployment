{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94c429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "warehouse_location = 'hdfs://hdfs:9000/hive/warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cdac693",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Id', IntegerType(), True),\n",
    "           StructField('SepalLengthCm', DoubleType(), True),\n",
    "           StructField('SepalWidthCm', DoubleType(), True),\n",
    "           StructField('PetalLengthCm', DoubleType(), True),\n",
    "           StructField('PetalWidthCm', DoubleType(), True),\n",
    "           StructField('Species', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96873c59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/22 13:03:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f5bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 13:04:13 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/12/22 13:04:13 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/12/22 13:04:16 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/12/22 13:04:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/12/22 13:04:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/12/22 13:04:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS iris (Id INT, SepalLengthCm DOUBLE, SepalWidthCm DOUBLE, PetalLengthCm DOUBLE, PetalWidthCm DOUBLE, Species STRING) USING hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e18e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"CSV\").option(\"header\",\"true\").schema(schema).load('hdfs://hdfs:9000/user/Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c41534b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c782248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 'int'),\n",
       " ('SepalLengthCm', 'double'),\n",
       " ('SepalWidthCm', 'double'),\n",
       " ('PetalLengthCm', 'double'),\n",
       " ('PetalWidthCm', 'double'),\n",
       " ('Species', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243a958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 15:14:59 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!\n",
      "21/12/22 15:14:59 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "df.write.insertInto('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c14ff476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max(PetalWidthCm)|\n",
      "+-----------------+\n",
      "|              2.5|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select max(PetalWidthCm) from iris').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "790b36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 3.simple-hive.ipynb to script\n",
      "[NbConvertApp] Writing 1247 bytes to 3.simple-hive.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 3.simple-hive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57b30b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/12/22 15:17:14 INFO SparkContext: Running Spark version 3.2.0\n",
      "21/12/22 15:17:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/22 15:17:15 INFO ResourceUtils: ==============================================================\n",
      "21/12/22 15:17:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/12/22 15:17:15 INFO ResourceUtils: ==============================================================\n",
      "21/12/22 15:17:15 INFO SparkContext: Submitted application: Python Spark SQL Hive integration example\n",
      "21/12/22 15:17:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/12/22 15:17:15 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/12/22 15:17:15 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/12/22 15:17:15 INFO SecurityManager: Changing view acls to: root\n",
      "21/12/22 15:17:15 INFO SecurityManager: Changing modify acls to: root\n",
      "21/12/22 15:17:15 INFO SecurityManager: Changing view acls groups to: \n",
      "21/12/22 15:17:15 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/12/22 15:17:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "21/12/22 15:17:16 INFO Utils: Successfully started service 'sparkDriver' on port 36049.\n",
      "21/12/22 15:17:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/12/22 15:17:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/12/22 15:17:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/12/22 15:17:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/12/22 15:17:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/12/22 15:17:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1439439f-da55-4a9f-b977-11badcc01619\n",
      "21/12/22 15:17:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/12/22 15:17:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/12/22 15:17:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/22 15:17:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/12/22 15:17:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://01942d2d4bbb:4041\n",
      "21/12/22 15:17:17 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...\n",
      "21/12/22 15:17:17 INFO TransportClientFactory: Successfully created connection to master/172.20.0.4:7077 after 65 ms (0 ms spent in bootstraps)\n",
      "21/12/22 15:17:17 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20211222151717-0000\n",
      "21/12/22 15:17:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45409.\n",
      "21/12/22 15:17:17 INFO NettyBlockTransferService: Server created on 01942d2d4bbb:45409\n",
      "21/12/22 15:17:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/12/22 15:17:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 01942d2d4bbb, 45409, None)\n",
      "21/12/22 15:17:18 INFO BlockManagerMasterEndpoint: Registering block manager 01942d2d4bbb:45409 with 366.3 MiB RAM, BlockManagerId(driver, 01942d2d4bbb, 45409, None)\n",
      "21/12/22 15:17:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 01942d2d4bbb, 45409, None)\n",
      "21/12/22 15:17:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 01942d2d4bbb, 45409, None)\n",
      "21/12/22 15:17:18 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20211222151717-0000/0 on worker-20211222130125-172.20.0.5-35837 (172.20.0.5:35837) with 2 core(s)\n",
      "21/12/22 15:17:18 INFO StandaloneSchedulerBackend: Granted executor ID app-20211222151717-0000/0 on hostPort 172.20.0.5:35837 with 2 core(s), 1024.0 MiB RAM\n",
      "21/12/22 15:17:18 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "21/12/22 15:17:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20211222151717-0000/0 is now RUNNING\n",
      "21/12/22 15:17:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "21/12/22 15:17:21 INFO SharedState: Warehouse path is 'hdfs://hdfs:9000/hive/warehouse'.\n",
      "21/12/22 15:17:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.5:34254) with ID 0,  ResourceProfileId 0\n",
      "21/12/22 15:17:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.5:41523 with 366.3 MiB RAM, BlockManagerId(0, 172.20.0.5, 41523, None)\n",
      "21/12/22 15:17:28 INFO HiveConf: Found configuration file file:/opt/spark/conf/hive-site.xml\n",
      "21/12/22 15:17:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
      "21/12/22 15:17:28 INFO HiveConf: Found configuration file file:/opt/spark/conf/hive-site.xml\n",
      "21/12/22 15:17:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is hdfs://hdfs:9000/hive/warehouse\n",
      "21/12/22 15:17:30 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/12/22 15:17:30 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/12/22 15:17:30 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "21/12/22 15:17:30 INFO ObjectStore: ObjectStore, initialize called\n",
      "21/12/22 15:17:30 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored\n",
      "21/12/22 15:17:30 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "21/12/22 15:17:30 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "21/12/22 15:17:31 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "21/12/22 15:17:32 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES\n",
      "21/12/22 15:17:32 INFO ObjectStore: Initialized ObjectStore\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: Added admin role in metastore\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: Added public role in metastore\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: No user is added in admin role, since config is empty\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:33 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c6109d77-4959-4c47-bb41-678b1fcdff4c, clientType=HIVECLI]\n",
      "21/12/22 15:17:33 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/12/22 15:17:33 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=Cleaning up thread local RawStore...\t\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=Done cleaning up thread local RawStore\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: create_table: Table(tableName:iris, dbName:default, owner:root, createTime:1640186244, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:Id, type:int, comment:null), FieldSchema(name:SepalLengthCm, type:double, comment:null), FieldSchema(name:SepalWidthCm, type:double, comment:null), FieldSchema(name:PetalLengthCm, type:double, comment:null), FieldSchema(name:PetalWidthCm, type:double, comment:null), FieldSchema(name:Species, type:string, comment:null)], location:hdfs://hdfs:9000/user/warehouse/iris, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[{\"name\":\"Id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SepalLengthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SepalWidthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PetalLengthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PetalWidthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Species\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\n",
      "21/12/22 15:17:33 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=create_table: Table(tableName:iris, dbName:default, owner:root, createTime:1640186244, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:Id, type:int, comment:null), FieldSchema(name:SepalLengthCm, type:double, comment:null), FieldSchema(name:SepalWidthCm, type:double, comment:null), FieldSchema(name:PetalLengthCm, type:double, comment:null), FieldSchema(name:PetalWidthCm, type:double, comment:null), FieldSchema(name:Species, type:string, comment:null)], location:hdfs://hdfs:9000/user/warehouse/iris, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[{\"name\":\"Id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SepalLengthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SepalWidthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PetalLengthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PetalWidthCm\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Species\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\t\n",
      "21/12/22 15:17:33 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/12/22 15:17:33 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/12/22 15:17:33 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/12/22 15:17:33 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "21/12/22 15:17:33 INFO ObjectStore: ObjectStore, initialize called\n",
      "21/12/22 15:17:33 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES\n",
      "21/12/22 15:17:33 INFO ObjectStore: Initialized ObjectStore\n",
      "21/12/22 15:17:34 INFO InMemoryFileIndex: It took 77 ms to list leaf files for 1 paths.\n",
      "21/12/22 15:17:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 15:17:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/12/22 15:17:34 INFO FileSourceStrategy: Output Data Schema: struct<Id: int, SepalLengthCm: double, SepalWidthCm: double, PetalLengthCm: double, PetalWidthCm: double ... 1 more field>\n",
      "21/12/22 15:17:35 INFO CodeGenerator: Code generated in 408.568448 ms\n",
      "21/12/22 15:17:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 289.9 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 01942d2d4bbb:45409 (size: 24.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:36 INFO SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 15:17:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 15:17:36 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 15:17:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.6 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 01942d2d4bbb:45409 (size: 7.4 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 15:17:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 15:17:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/12/22 15:17:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.5, executor 0, partition 0, ANY, 4859 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.5:41523 (size: 7.4 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.5:41523 (size: 24.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3996 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 15:17:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/12/22 15:17:40 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 4.223 s\n",
      "21/12/22 15:17:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 15:17:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 15:17:40 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 4.359156 s\n",
      "21/12/22 15:17:40 INFO CodeGenerator: Code generated in 67.370373 ms\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "21/12/22 15:17:41 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:41 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO Persistence: Request to load fields \"comment,name,type\" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored\n",
      "21/12/22 15:17:41 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:41 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:41 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:41 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 15:17:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/12/22 15:17:41 INFO FileSourceStrategy: Output Data Schema: struct<Id: int, SepalLengthCm: double, SepalWidthCm: double, PetalLengthCm: double, PetalWidthCm: double ... 1 more field>\n",
      "21/12/22 15:17:41 INFO FileUtils: Creating directory if it doesn't exist: hdfs://hdfs:9000/user/warehouse/iris/.hive-staging_hive_2021-12-22_15-17-41_276_927058668569342969-1\n",
      "21/12/22 15:17:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "21/12/22 15:17:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "21/12/22 15:17:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 289.9 KiB, free 365.7 MiB)\n",
      "21/12/22 15:17:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 365.7 MiB)\n",
      "21/12/22 15:17:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 01942d2d4bbb:45409 (size: 24.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:41 INFO SparkContext: Created broadcast 2 from insertInto at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 15:17:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 15:17:41 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 15:17:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 228.4 KiB, free 365.4 MiB)\n",
      "21/12/22 15:17:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.3 KiB, free 365.4 MiB)\n",
      "21/12/22 15:17:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 01942d2d4bbb:45409 (size: 82.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 15:17:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 15:17:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/12/22 15:17:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.20.0.5, executor 0, partition 0, ANY, 4859 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.20.0.5:41523 (size: 82.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.20.0.5:41523 (size: 24.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1016 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 15:17:42 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.087 s\n",
      "21/12/22 15:17:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 15:17:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/12/22 15:17:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/12/22 15:17:42 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.098437 s\n",
      "21/12/22 15:17:42 INFO FileFormatWriter: Start to commit write Job 47f0c6d7-68cf-4c33-a14c-5b3276aa0a37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 15:17:42 INFO FileFormatWriter: Write Job 47f0c6d7-68cf-4c33-a14c-5b3276aa0a37 committed. Elapsed time: 66 ms.\n",
      "21/12/22 15:17:42 INFO FileFormatWriter: Finished processing stats for write job 47f0c6d7-68cf-4c33-a14c-5b3276aa0a37.\n",
      "21/12/22 15:17:42 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:42 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:42 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:42 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:42 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: alter_table: db=default tbl=iris newtbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=alter_table: db=default tbl=iris newtbl=iris\t\n",
      "21/12/22 15:17:43 INFO log: Updating table stats fast for iris\n",
      "21/12/22 15:17:43 INFO log: Updated size of table iris to 15190\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_database: global_temp\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: global_temp\t\n",
      "21/12/22 15:17:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: alter_table: db=default tbl=iris newtbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=alter_table: db=default tbl=iris newtbl=iris\t\n",
      "21/12/22 15:17:43 INFO log: Updating table stats fast for iris\n",
      "21/12/22 15:17:43 INFO log: Updated size of table iris to 15190\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_database: default\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:43 INFO HiveMetaStore: 0: get_table : db=default tbl=iris\n",
      "21/12/22 15:17:43 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=iris\t\n",
      "21/12/22 15:17:44 INFO CodeGenerator: Code generated in 33.410811 ms\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.20.0.5:41523 in memory (size: 24.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 01942d2d4bbb:45409 in memory (size: 24.3 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 290.8 KiB, free 365.4 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.20.0.5:41523 in memory (size: 82.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 01942d2d4bbb:45409 in memory (size: 82.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 365.7 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 01942d2d4bbb:45409 (size: 24.4 KiB, free: 366.2 MiB)\n",
      "21/12/22 15:17:44 INFO SparkContext: Created broadcast 4 from \n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.20.0.5:41523 in memory (size: 7.4 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 01942d2d4bbb:45409 in memory (size: 7.4 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.20.0.5:41523 in memory (size: 24.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 01942d2d4bbb:45409 in memory (size: 24.3 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO FileInputFormat: Total input paths to process : 3\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Registering RDD 11 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Got map stage job 2 (showString at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 15:17:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.5 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 01942d2d4bbb:45409 (size: 8.1 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 15:17:44 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "21/12/22 15:17:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
      "21/12/22 15:17:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.20.0.5, executor 0, partition 0, ANY, 4507 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:44 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.20.0.5, executor 0, partition 1, ANY, 4551 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.20.0.5:41523 (size: 8.1 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.20.0.5:41523 (size: 24.4 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:45 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (172.20.0.5, executor 0, partition 2, ANY, 4551 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:45 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 675 ms on 172.20.0.5 (executor 0) (1/3)\n",
      "21/12/22 15:17:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 713 ms on 172.20.0.5 (executor 0) (2/3)\n",
      "21/12/22 15:17:45 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 82 ms on 172.20.0.5 (executor 0) (3/3)\n",
      "21/12/22 15:17:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/12/22 15:17:45 INFO DAGScheduler: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.843 s\n",
      "21/12/22 15:17:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/12/22 15:17:45 INFO DAGScheduler: running: Set()\n",
      "21/12/22 15:17:45 INFO DAGScheduler: waiting: Set()\n",
      "21/12/22 15:17:45 INFO DAGScheduler: failed: Set()\n",
      "21/12/22 15:17:45 INFO CodeGenerator: Code generated in 28.832098 ms\n",
      "21/12/22 15:17:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 15:17:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:45 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "21/12/22 15:17:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 01942d2d4bbb:45409 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:45 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 15:17:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 15:17:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 15:17:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.20.0.5, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "21/12/22 15:17:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.20.0.5:41523 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "21/12/22 15:17:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.5:34254\n",
      "21/12/22 15:17:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 466 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 15:17:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "21/12/22 15:17:46 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.493 s\n",
      "21/12/22 15:17:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 15:17:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "21/12/22 15:17:46 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.514681 s\n",
      "21/12/22 15:17:46 INFO CodeGenerator: Code generated in 41.872028 ms\n",
      "+-----------------+\n",
      "|max(PetalWidthCm)|\n",
      "+-----------------+\n",
      "|              2.5|\n",
      "+-----------------+\n",
      "\n",
      "21/12/22 15:17:46 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "21/12/22 15:17:46 INFO SparkUI: Stopped Spark web UI at http://01942d2d4bbb:4041\n",
      "21/12/22 15:17:46 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "21/12/22 15:17:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "21/12/22 15:17:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/12/22 15:17:46 INFO MemoryStore: MemoryStore cleared\n",
      "21/12/22 15:17:46 INFO BlockManager: BlockManager stopped\n",
      "21/12/22 15:17:46 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/12/22 15:17:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/12/22 15:17:46 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/12/22 15:17:46 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/12/22 15:17:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-f46a9cf1-9c66-4e02-8be8-1a234799dd26\n",
      "21/12/22 15:17:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-f46a9cf1-9c66-4e02-8be8-1a234799dd26/pyspark-8765a5ce-7e0a-4d33-9bfb-83d36eb58568\n",
      "21/12/22 15:17:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-05f1e40b-db6b-47b0-965e-56722adbb376\n"
     ]
    }
   ],
   "source": [
    "!/opt/spark/bin/spark-submit \\\n",
    "  --master spark://master:7077 \\\n",
    "  /home/data/3.simple-hive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70aa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
