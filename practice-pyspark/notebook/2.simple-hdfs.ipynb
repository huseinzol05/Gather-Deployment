{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025a8958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/22 08:10:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/22 08:10:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/22 08:10:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"csv\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dd8514",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://hdfs:9000/user/Iris.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_785/3498114954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs://hdfs:9000/user/Iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://hdfs:9000/user/Iris.csv"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.format(\"CSV\").option(\"header\",\"true\").load('hdfs://hdfs:9000/user/Iris.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867acdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af04b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(SepalLengthCm)|\n",
      "+------------------+\n",
      "|               7.9|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(max('SepalLengthCm')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4a09adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2.simple-hdfs.ipynb to script\n",
      "[NbConvertApp] Writing 591 bytes to 2.simple-hdfs.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 2.simple-hdfs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cfcbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/12/22 03:22:51 INFO SparkContext: Running Spark version 3.2.0\n",
      "21/12/22 03:22:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/22 03:22:51 INFO ResourceUtils: ==============================================================\n",
      "21/12/22 03:22:51 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/12/22 03:22:51 INFO ResourceUtils: ==============================================================\n",
      "21/12/22 03:22:51 INFO SparkContext: Submitted application: csv\n",
      "21/12/22 03:22:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/12/22 03:22:52 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/12/22 03:22:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/12/22 03:22:52 INFO SecurityManager: Changing view acls to: root\n",
      "21/12/22 03:22:52 INFO SecurityManager: Changing modify acls to: root\n",
      "21/12/22 03:22:52 INFO SecurityManager: Changing view acls groups to: \n",
      "21/12/22 03:22:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/12/22 03:22:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "21/12/22 03:22:52 INFO Utils: Successfully started service 'sparkDriver' on port 44981.\n",
      "21/12/22 03:22:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/12/22 03:22:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/12/22 03:22:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/12/22 03:22:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/12/22 03:22:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/12/22 03:22:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1c5c3f27-4eb0-4750-9d94-fb0611ed7e8c\n",
      "21/12/22 03:22:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/12/22 03:22:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/12/22 03:22:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/22 03:22:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/12/22 03:22:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://24913cdd3bfd:4041\n",
      "21/12/22 03:22:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...\n",
      "21/12/22 03:22:54 INFO TransportClientFactory: Successfully created connection to master/172.20.0.4:7077 after 61 ms (0 ms spent in bootstraps)\n",
      "21/12/22 03:22:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20211222032254-0000\n",
      "21/12/22 03:22:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39319.\n",
      "21/12/22 03:22:54 INFO NettyBlockTransferService: Server created on 24913cdd3bfd:39319\n",
      "21/12/22 03:22:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/12/22 03:22:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 24913cdd3bfd, 39319, None)\n",
      "21/12/22 03:22:54 INFO BlockManagerMasterEndpoint: Registering block manager 24913cdd3bfd:39319 with 366.3 MiB RAM, BlockManagerId(driver, 24913cdd3bfd, 39319, None)\n",
      "21/12/22 03:22:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 24913cdd3bfd, 39319, None)\n",
      "21/12/22 03:22:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 24913cdd3bfd, 39319, None)\n",
      "21/12/22 03:22:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20211222032254-0000/0 on worker-20211222025822-172.20.0.5-43345 (172.20.0.5:43345) with 2 core(s)\n",
      "21/12/22 03:22:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20211222032254-0000/0 on hostPort 172.20.0.5:43345 with 2 core(s), 1024.0 MiB RAM\n",
      "21/12/22 03:22:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20211222032254-0000/0 is now RUNNING\n",
      "21/12/22 03:22:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "21/12/22 03:22:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "21/12/22 03:22:56 INFO SharedState: Warehouse path is 'file:/home/data/spark-warehouse'.\n",
      "21/12/22 03:23:00 INFO InMemoryFileIndex: It took 146 ms to list leaf files for 1 paths.\n",
      "21/12/22 03:23:00 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.\n",
      "21/12/22 03:23:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.5:39716) with ID 0,  ResourceProfileId 0\n",
      "21/12/22 03:23:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.5:34887 with 366.3 MiB RAM, BlockManagerId(0, 172.20.0.5, 34887, None)\n",
      "21/12/22 03:23:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 03:23:05 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "21/12/22 03:23:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/12/22 03:23:06 INFO CodeGenerator: Code generated in 371.960205 ms\n",
      "21/12/22 03:23:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 287.0 KiB, free 366.0 MiB)\n",
      "21/12/22 03:23:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 366.0 MiB)\n",
      "21/12/22 03:23:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 24913cdd3bfd:39319 (size: 24.2 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:06 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 03:23:07 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 03:23:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 366.0 MiB)\n",
      "21/12/22 03:23:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "21/12/22 03:23:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 24913cdd3bfd:39319 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 03:23:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 03:23:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/12/22 03:23:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.5, executor 0, partition 0, ANY, 4859 bytes) taskResourceAssignments Map()\n",
      "21/12/22 03:23:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.5:34887 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.5:34887 (size: 24.2 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3866 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 03:23:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/12/22 03:23:11 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.036 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 03:23:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 03:23:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.141854 s\n",
      "21/12/22 03:23:11 INFO CodeGenerator: Code generated in 48.316765 ms\n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 287.0 KiB, free 365.7 MiB)\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 365.7 MiB)\n",
      "21/12/22 03:23:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 24913cdd3bfd:39319 (size: 24.2 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:11 INFO SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/12/22 03:23:11 INFO FileSourceStrategy: Output Data Schema: struct<Id: string, SepalLengthCm: string, SepalWidthCm: string, PetalLengthCm: string, PetalWidthCm: string ... 1 more field>\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 286.8 KiB, free 365.4 MiB)\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 365.4 MiB)\n",
      "21/12/22 03:23:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 24913cdd3bfd:39319 (size: 24.1 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:11 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 03:23:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.4 KiB, free 365.4 MiB)\n",
      "21/12/22 03:23:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.4 MiB)\n",
      "21/12/22 03:23:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 24913cdd3bfd:39319 (size: 5.6 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 03:23:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 03:23:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/12/22 03:23:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.20.0.5, executor 0, partition 0, ANY, 4859 bytes) taskResourceAssignments Map()\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.20.0.5:34887 (size: 5.6 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.20.0.5:34887 (size: 24.1 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 599 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 03:23:12 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.656 s\n",
      "21/12/22 03:23:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 03:23:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/12/22 03:23:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/12/22 03:23:12 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.673168 s\n",
      "21/12/22 03:23:12 INFO CodeGenerator: Code generated in 59.189161 ms\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n",
      "| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n",
      "| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n",
      "| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n",
      "| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n",
      "| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n",
      "| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n",
      "| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n",
      "| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n",
      "| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.20.0.5:34887 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 24913cdd3bfd:39319 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/12/22 03:23:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/12/22 03:23:12 INFO FileSourceStrategy: Output Data Schema: struct<SepalLengthCm: string>\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 24913cdd3bfd:39319 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.20.0.5:34887 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "21/12/22 03:23:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 286.8 KiB, free 365.1 MiB)\n",
      "21/12/22 03:23:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 365.1 MiB)\n",
      "21/12/22 03:23:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 24913cdd3bfd:39319 (size: 24.1 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:13 INFO SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Registering RDD 16 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Got map stage job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/12/22 03:23:13 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.6 KiB, free 365.1 MiB)\n",
      "21/12/22 03:23:13 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 365.1 MiB)\n",
      "21/12/22 03:23:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 24913cdd3bfd:39319 (size: 10.7 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:13 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 03:23:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 03:23:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "21/12/22 03:23:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.20.0.5, executor 0, partition 0, ANY, 4848 bytes) taskResourceAssignments Map()\n",
      "21/12/22 03:23:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.20.0.5:34887 (size: 10.7 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.20.0.5:34887 (size: 24.1 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 700 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 03:23:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/12/22 03:23:13 INFO DAGScheduler: ShuffleMapStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.793 s\n",
      "21/12/22 03:23:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/12/22 03:23:13 INFO DAGScheduler: running: Set()\n",
      "21/12/22 03:23:13 INFO DAGScheduler: waiting: Set()\n",
      "21/12/22 03:23:13 INFO DAGScheduler: failed: Set()\n",
      "21/12/22 03:23:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/12/22 03:23:14 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 25.0 KiB, free 365.0 MiB)\n",
      "21/12/22 03:23:14 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.0 MiB)\n",
      "21/12/22 03:23:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 24913cdd3bfd:39319 (size: 12.4 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:14 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/12/22 03:23:14 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "21/12/22 03:23:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.20.0.5, executor 0, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "21/12/22 03:23:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.20.0.5:34887 (size: 12.4 KiB, free: 366.2 MiB)\n",
      "21/12/22 03:23:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.5:39716\n",
      "21/12/22 03:23:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 491 ms on 172.20.0.5 (executor 0) (1/1)\n",
      "21/12/22 03:23:14 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.507 s\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/22 03:23:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "21/12/22 03:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "21/12/22 03:23:14 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.528470 s\n",
      "21/12/22 03:23:14 INFO CodeGenerator: Code generated in 14.486984 ms\n",
      "+------------------+\n",
      "|max(SepalLengthCm)|\n",
      "+------------------+\n",
      "|               7.9|\n",
      "+------------------+\n",
      "\n",
      "21/12/22 03:23:14 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "21/12/22 03:23:14 INFO SparkUI: Stopped Spark web UI at http://24913cdd3bfd:4041\n",
      "21/12/22 03:23:14 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "21/12/22 03:23:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "21/12/22 03:23:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/12/22 03:23:14 INFO MemoryStore: MemoryStore cleared\n",
      "21/12/22 03:23:14 INFO BlockManager: BlockManager stopped\n",
      "21/12/22 03:23:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/12/22 03:23:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/12/22 03:23:14 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/12/22 03:23:14 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/12/22 03:23:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d65b8a8-43d5-4cac-a398-2b6449075109\n",
      "21/12/22 03:23:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-51207cee-65c6-4df6-a749-c84f83b904c1\n",
      "21/12/22 03:23:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d65b8a8-43d5-4cac-a398-2b6449075109/pyspark-c728fea6-0490-4817-8387-dbf7970d6286\n"
     ]
    }
   ],
   "source": [
    "!/opt/spark/bin/spark-submit \\\n",
    "  --master spark://master:7077 \\\n",
    "  /home/data/2.simple-hdfs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96992ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
