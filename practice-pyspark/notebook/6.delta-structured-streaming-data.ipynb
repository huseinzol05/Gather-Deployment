{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7f474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/drabastomek/learningPySpark/master/Chapter03/flight-data/departuredelays.csv\n",
    "# !$HADOOP_HOME/bin/hdfs dfs -put departuredelays.csv hdfs://hdfs:9000/user/flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b03b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7611d04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a50e45c4-c644-4350-9ccc-71196f615e17;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 615ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a50e45c4-c644-4350-9ccc-71196f615e17\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/19ms)\n",
      "22/01/21 12:42:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "warehouse_location = 'hdfs://hdfs:9000/hive/warehouse'\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('spark://master:7077') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373351e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !$HADOOP_HOME/bin/hdfs dfs -rm -r hdfs://hdfs:9000/user/flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee09356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fulldf = spark.read.format('csv').option('header', True).option('inferSchema', True) \\\n",
    ".load('hdfs://hdfs:9000/user/flight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22241e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fulldf.write.format('delta').mode('overwrite').saveAsTable(\"default.full_flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49e7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = fulldf.limit(10)\n",
    "df = (df.withColumn('primaryKey', expr(\"SHA2(concat_ws(',', date,origin,destination),512)\")) \\\n",
    "     .withColumn('changeKey', expr(\"SHA2(concat_ws(',', delay,destination),512)\")))\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('default.small_flight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04defcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|   date|delay|distance|origin|destination|          primaryKey|           changeKey|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|1011245|    6|     602|   ABE|        ATL|162b85d8031aee4c0...|d463593f979122fd5...|\n",
      "|1020600|   -8|     369|   ABE|        DTW|c19de07f14d54bb3c...|3fa1eb583606a3101...|\n",
      "|1021245|   -2|     602|   ABE|        ATL|0939e101ee38e590b...|bbb9f6468e7533511...|\n",
      "|1020605|   -4|     602|   ABE|        ATL|74015a471b259ec46...|3b55261eff1f92eb8...|\n",
      "|1031245|   -4|     602|   ABE|        ATL|66603a6adf37e66bc...|3b55261eff1f92eb8...|\n",
      "|1030605|    0|     602|   ABE|        ATL|2f6d059b814f81b0e...|1b99e067be9bcbdfa...|\n",
      "|1041243|   10|     602|   ABE|        ATL|d62deb643c40d25cc...|aa03be7da41aecc61...|\n",
      "|1040605|   28|     602|   ABE|        ATL|af3f60816d7d9d323...|0b3809e2a4a9e94ea...|\n",
      "|1051245|   88|     602|   ABE|        ATL|e7b67b7f2069aa0c8...|8d6c7dca2064fbb91...|\n",
      "|1050605|    9|     602|   ABE|        ATL|3803f58b13830e879...|e9872741dee48c4d6...|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8ab421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                me version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|   date|delay|distance|origin|destination|          primaryKey|           changeKey|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|1021245|   -2|     602|   ABE|        ATL|0939e101ee38e590b...|bbb9f6468e7533511...|\n",
      "|1011245|    6|     602|   ABE|        ATL|162b85d8031aee4c0...|d463593f979122fd5...|\n",
      "|1030605|    0|     602|   ABE|        ATL|2f6d059b814f81b0e...|1b99e067be9bcbdfa...|\n",
      "|1050605|    9|     602|   ABE|        ATL|3803f58b13830e879...|e9872741dee48c4d6...|\n",
      "|1031245|   -4|     602|   ABE|        ATL|66603a6adf37e66bc...|3b55261eff1f92eb8...|\n",
      "|1020605|   -4|     602|   ABE|        ATL|74015a471b259ec46...|3b55261eff1f92eb8...|\n",
      "|1040605|   28|     602|   ABE|        ATL|af3f60816d7d9d323...|0b3809e2a4a9e94ea...|\n",
      "|1020600|   -8|     369|   ABE|        DTW|c19de07f14d54bb3c...|3fa1eb583606a3101...|\n",
      "|1041243|   10|     602|   ABE|        ATL|d62deb643c40d25cc...|aa03be7da41aecc61...|\n",
      "|1051245|   88|     602|   ABE|        ATL|e7b67b7f2069aa0c8...|8d6c7dca2064fbb91...|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from default.small_flight order by primaryKey').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ba4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "insert into default.small_flight\n",
    "select date, delay, 999 distance, origin, destination,\n",
    "SHA2(concat_ws(',', date,origin,destination),512) as primaryKey,\n",
    "SHA2(concat_ws(',', delay,destination),512) as changeKey\n",
    "from default.full_flight order by rand() limit 10\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cabe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|   date|delay|distance|origin|destination|          primaryKey|           changeKey|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "|1021245|   -2|     602|   ABE|        ATL|0939e101ee38e590b...|bbb9f6468e7533511...|\n",
      "|1011245|    6|     602|   ABE|        ATL|162b85d8031aee4c0...|d463593f979122fd5...|\n",
      "|3060730|   -4|     999|   BWI|        STL|17ec14109192b11b7...|f97817e1400219fc1...|\n",
      "|1030605|    0|     602|   ABE|        ATL|2f6d059b814f81b0e...|1b99e067be9bcbdfa...|\n",
      "|1050605|    9|     602|   ABE|        ATL|3803f58b13830e879...|e9872741dee48c4d6...|\n",
      "|1091640|  188|     999|   BMI|        DFW|5ddea3c02874d8dc1...|77898cb32e0ff19ce...|\n",
      "|1031245|   -4|     602|   ABE|        ATL|66603a6adf37e66bc...|3b55261eff1f92eb8...|\n",
      "|1161240|    0|     999|   IND|        ORD|689ab3fd001444fa2...|311d56e4551b0b9b8...|\n",
      "|2040610|    3|     999|   SEA|        LAX|6c0a62ac1947c5190...|53726986aec709144...|\n",
      "|1020605|   -4|     602|   ABE|        ATL|74015a471b259ec46...|3b55261eff1f92eb8...|\n",
      "|1131008|   -6|     999|   IAH|        SAV|7bcc161e898b5ac76...|850da9f5b7960320d...|\n",
      "|1261900|  -10|     999|   IAD|        DFW|8749694c47e474e1e...|9e8ed048d03c3c0a8...|\n",
      "|3061737|    0|     999|   JAX|        ORD|9c7feeebb395aed47...|311d56e4551b0b9b8...|\n",
      "|2061715|    0|     999|   PHL|        BOS|a960ac6ee0978fa7f...|fb4690b589904b3f2...|\n",
      "|1040605|   28|     602|   ABE|        ATL|af3f60816d7d9d323...|0b3809e2a4a9e94ea...|\n",
      "|1020600|   -8|     369|   ABE|        DTW|c19de07f14d54bb3c...|3fa1eb583606a3101...|\n",
      "|1301530|   20|     999|   IND|        LAS|ca05928091b284fa8...|dbd7c956b44a97cd2...|\n",
      "|1041243|   10|     602|   ABE|        ATL|d62deb643c40d25cc...|aa03be7da41aecc61...|\n",
      "|2171247|    0|     999|   EUG|        SFO|dbd2e632006267745...|ed8f3c4e36e7fc955...|\n",
      "|1051245|   88|     602|   ABE|        ATL|e7b67b7f2069aa0c8...|8d6c7dca2064fbb91...|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from default.small_flight order by primaryKey').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe61d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "insert into default.small_flight\n",
    "select date, delay, 999 distance, origin, destination,\n",
    "SHA2(concat_ws(',', date,origin,destination),512) as primaryKey,\n",
    "SHA2(concat_ws(',', delay,destination),512) as changeKey\n",
    "from default.small_flight order by rand() limit 10\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d3f4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string, primaryKey: string, changeKey: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.readStream.format('delta').table('default.small_flight')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e3ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('inserted', current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "183acb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveloc = 'hdfs://hdfs:9000/user/flight/stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2280ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://hdfs:9000/user/flight/stream\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -rm -r hdfs://hdfs:9000/user/flight/stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aea7b52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 12:46:06 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[hdfs://hdfs:9000/hive/warehouse/small_flight] is ignored when Trigger.Once() is used.\n"
     ]
    }
   ],
   "source": [
    "streamQuery = df.writeStream.format('delta').outputMode('append') \\\n",
    ".option('checkpointLocation', f'{saveloc}/_checkpoint') \\\n",
    ".trigger(once=True) \\\n",
    ".start(saveloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72e47551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'dc123ca2-da02-45e6-b653-7db332c78ee0',\n",
       "  'runId': 'c8b3f419-9c21-45e9-a96c-6dc2a34d51a6',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-01-21T12:46:06.913Z',\n",
       "  'batchId': 0,\n",
       "  'numInputRows': 30,\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'processedRowsPerSecond': 5.056463846283499,\n",
       "  'durationMs': {'addBatch': 3626,\n",
       "   'getBatch': 14,\n",
       "   'latestOffset': 2081,\n",
       "   'queryPlanning': 58,\n",
       "   'triggerExecution': 5933,\n",
       "   'walCommit': 94},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'DeltaSource[hdfs://hdfs:9000/hive/warehouse/small_flight]',\n",
       "    'startOffset': None,\n",
       "    'endOffset': {'sourceVersion': 1,\n",
       "     'reservoirId': '32435ec1-cc7e-4706-8951-bf0a570a4095',\n",
       "     'reservoirVersion': 14,\n",
       "     'index': 2,\n",
       "     'isStartingVersion': True},\n",
       "    'numInputRows': 30,\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'processedRowsPerSecond': 5.056463846283499}],\n",
       "  'sink': {'description': 'DeltaSink[hdfs://hdfs:9000/user/flight/stream]',\n",
       "   'numOutputRows': -1}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1743a358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "drop table if exists default.flightstream;\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "sql = f\"\"\"\n",
    "create table default.flightstream using delta location '{saveloc}';\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a635f16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------------------+--------------------+--------------------+\n",
      "|   date|delay|distance|origin|destination|          primaryKey|           changeKey|            inserted|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+--------------------+\n",
      "|1021245|   -2|     999|   ABE|        ATL|0939e101ee38e590b...|bbb9f6468e7533511...|2022-01-21 12:46:...|\n",
      "|1021245|   -2|     602|   ABE|        ATL|0939e101ee38e590b...|bbb9f6468e7533511...|2022-01-21 12:46:...|\n",
      "|1011245|    6|     999|   ABE|        ATL|162b85d8031aee4c0...|d463593f979122fd5...|2022-01-21 12:46:...|\n",
      "|1011245|    6|     602|   ABE|        ATL|162b85d8031aee4c0...|d463593f979122fd5...|2022-01-21 12:46:...|\n",
      "|3060730|   -4|     999|   BWI|        STL|17ec14109192b11b7...|f97817e1400219fc1...|2022-01-21 12:46:...|\n",
      "|3060730|   -4|     999|   BWI|        STL|17ec14109192b11b7...|f97817e1400219fc1...|2022-01-21 12:46:...|\n",
      "|1030605|    0|     999|   ABE|        ATL|2f6d059b814f81b0e...|1b99e067be9bcbdfa...|2022-01-21 12:46:...|\n",
      "|1030605|    0|     602|   ABE|        ATL|2f6d059b814f81b0e...|1b99e067be9bcbdfa...|2022-01-21 12:46:...|\n",
      "|1050605|    9|     602|   ABE|        ATL|3803f58b13830e879...|e9872741dee48c4d6...|2022-01-21 12:46:...|\n",
      "|1091640|  188|     999|   BMI|        DFW|5ddea3c02874d8dc1...|77898cb32e0ff19ce...|2022-01-21 12:46:...|\n",
      "|1031245|   -4|     602|   ABE|        ATL|66603a6adf37e66bc...|3b55261eff1f92eb8...|2022-01-21 12:46:...|\n",
      "|1161240|    0|     999|   IND|        ORD|689ab3fd001444fa2...|311d56e4551b0b9b8...|2022-01-21 12:46:...|\n",
      "|2040610|    3|     999|   SEA|        LAX|6c0a62ac1947c5190...|53726986aec709144...|2022-01-21 12:46:...|\n",
      "|2040610|    3|     999|   SEA|        LAX|6c0a62ac1947c5190...|53726986aec709144...|2022-01-21 12:46:...|\n",
      "|1020605|   -4|     602|   ABE|        ATL|74015a471b259ec46...|3b55261eff1f92eb8...|2022-01-21 12:46:...|\n",
      "|1131008|   -6|     999|   IAH|        SAV|7bcc161e898b5ac76...|850da9f5b7960320d...|2022-01-21 12:46:...|\n",
      "|1131008|   -6|     999|   IAH|        SAV|7bcc161e898b5ac76...|850da9f5b7960320d...|2022-01-21 12:46:...|\n",
      "|1261900|  -10|     999|   IAD|        DFW|8749694c47e474e1e...|9e8ed048d03c3c0a8...|2022-01-21 12:46:...|\n",
      "|1261900|  -10|     999|   IAD|        DFW|8749694c47e474e1e...|9e8ed048d03c3c0a8...|2022-01-21 12:46:...|\n",
      "|3061737|    0|     999|   JAX|        ORD|9c7feeebb395aed47...|311d56e4551b0b9b8...|2022-01-21 12:46:...|\n",
      "+-------+-----+--------+------+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from default.flightstream order by primaryKey').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61419759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "insert into default.small_flight\n",
    "select date, delay, 999 distance, origin, destination,\n",
    "SHA2(concat_ws(',', date,origin,destination),512) as primaryKey,\n",
    "SHA2(concat_ws(',', delay,destination),512) as changeKey\n",
    "from default.small_flight order by rand() limit 10\n",
    "\"\"\"\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba977ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fd0984d42e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltadf = DeltaTable.forName(spark, 'default.flightstream')\n",
    "deltadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1624830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(microdf, batchId):\n",
    "    print(f'batch id: {batchId}, rows count: {microdf.count()}')\n",
    "    microdf = microdf.dropDuplicates(['primaryKey', 'changeKey'])\n",
    "    deltadf.alias('t').merge(microdf.alias('s'),'s.primaryKey = t.primaryKey') \\\n",
    "    .whenMatchedUpdateAll('s.changeKey <> t.changeKey') \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8217f436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 12:46:23 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[hdfs://hdfs:9000/hive/warehouse/small_flight] is ignored when Trigger.Once() is used.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch id: 1, rows count: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 12:46:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 104:====================>                                 (75 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "streamQuery = df.writeStream.format('delta').outputMode('append') \\\n",
    ".foreachBatch(merge) \\\n",
    ".option('checkpointLocation', f'{saveloc}/_checkpoint') \\\n",
    ".trigger(once=True) \\\n",
    ".start(saveloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36ae1f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:==========>                                           (39 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "streamQuery.recentProgress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
