{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f932027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/19 14:51:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563de1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14167856\n"
     ]
    }
   ],
   "source": [
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426bf33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 0.test-pi.ipynb to script\n",
      "[NbConvertApp] Writing 379 bytes to 0.test-pi.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 0.test-pi.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e87d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/12/19 14:56:04 INFO SparkContext: Running Spark version 3.2.0\n",
      "21/12/19 14:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/19 14:56:04 INFO ResourceUtils: ==============================================================\n",
      "21/12/19 14:56:04 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/12/19 14:56:04 INFO ResourceUtils: ==============================================================\n",
      "21/12/19 14:56:04 INFO SparkContext: Submitted application: Pi\n",
      "21/12/19 14:56:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/12/19 14:56:04 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/12/19 14:56:04 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/12/19 14:56:04 INFO SecurityManager: Changing view acls to: root\n",
      "21/12/19 14:56:04 INFO SecurityManager: Changing modify acls to: root\n",
      "21/12/19 14:56:04 INFO SecurityManager: Changing view acls groups to: \n",
      "21/12/19 14:56:04 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/12/19 14:56:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "21/12/19 14:56:05 INFO Utils: Successfully started service 'sparkDriver' on port 36575.\n",
      "21/12/19 14:56:05 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/12/19 14:56:05 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/12/19 14:56:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/12/19 14:56:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/12/19 14:56:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/12/19 14:56:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12fcf4ad-d39c-4d0f-a638-6965f1c2bf1f\n",
      "21/12/19 14:56:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/12/19 14:56:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/12/19 14:56:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/12/19 14:56:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e65d5b46bbe5:4040\n",
      "21/12/19 14:56:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.20.0.3:7077...\n",
      "21/12/19 14:56:06 INFO TransportClientFactory: Successfully created connection to /172.20.0.3:7077 after 79 ms (0 ms spent in bootstraps)\n",
      "21/12/19 14:56:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20211219145607-0000\n",
      "21/12/19 14:56:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42019.\n",
      "21/12/19 14:56:07 INFO NettyBlockTransferService: Server created on e65d5b46bbe5:42019\n",
      "21/12/19 14:56:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/12/19 14:56:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e65d5b46bbe5, 42019, None)\n",
      "21/12/19 14:56:07 INFO BlockManagerMasterEndpoint: Registering block manager e65d5b46bbe5:42019 with 366.3 MiB RAM, BlockManagerId(driver, e65d5b46bbe5, 42019, None)\n",
      "21/12/19 14:56:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e65d5b46bbe5, 42019, None)\n",
      "21/12/19 14:56:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e65d5b46bbe5, 42019, None)\n",
      "21/12/19 14:56:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20211219145607-0000/0 on worker-20211219145026-172.20.0.4-46179 (172.20.0.4:46179) with 2 core(s)\n",
      "21/12/19 14:56:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20211219145607-0000/0 on hostPort 172.20.0.4:46179 with 2 core(s), 1024.0 MiB RAM\n",
      "21/12/19 14:56:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20211219145607-0000/0 is now RUNNING\n",
      "21/12/19 14:56:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "21/12/19 14:56:10 INFO SparkContext: Starting job: count at /home/data/0.test-pi.py:20\n",
      "21/12/19 14:56:10 INFO DAGScheduler: Got job 0 (count at /home/data/0.test-pi.py:20) with 2 output partitions\n",
      "21/12/19 14:56:10 INFO DAGScheduler: Final stage: ResultStage 0 (count at /home/data/0.test-pi.py:20)\n",
      "21/12/19 14:56:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/12/19 14:56:10 INFO DAGScheduler: Missing parents: List()\n",
      "21/12/19 14:56:10 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at count at /home/data/0.test-pi.py:20), which has no missing parents\n",
      "21/12/19 14:56:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.1 KiB, free 366.3 MiB)\n",
      "21/12/19 14:56:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 366.3 MiB)\n",
      "21/12/19 14:56:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e65d5b46bbe5:42019 (size: 4.9 KiB, free: 366.3 MiB)\n",
      "21/12/19 14:56:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1427\n",
      "21/12/19 14:56:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at count at /home/data/0.test-pi.py:20) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/12/19 14:56:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "21/12/19 14:56:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.4:40726) with ID 0,  ResourceProfileId 0\n",
      "21/12/19 14:56:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.4:40771 with 366.3 MiB RAM, BlockManagerId(0, 172.20.0.4, 40771, None)\n",
      "21/12/19 14:56:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 4437 bytes) taskResourceAssignments Map()\n",
      "21/12/19 14:56:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.20.0.4, executor 0, partition 1, PROCESS_LOCAL, 4437 bytes) taskResourceAssignments Map()\n",
      "21/12/19 14:56:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.4:40771 (size: 4.9 KiB, free: 366.3 MiB)\n",
      "21/12/19 14:57:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 70738 ms on 172.20.0.4 (executor 0) (1/2)\n",
      "21/12/19 14:57:24 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52853\n",
      "21/12/19 14:57:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 71127 ms on 172.20.0.4 (executor 0) (2/2)\n",
      "21/12/19 14:57:25 INFO DAGScheduler: ResultStage 0 (count at /home/data/0.test-pi.py:20) finished in 74.214 s\n",
      "21/12/19 14:57:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/12/19 14:57:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/12/19 14:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/12/19 14:57:25 INFO DAGScheduler: Job 0 finished: count at /home/data/0.test-pi.py:20, took 74.387372 s\n",
      "3.1417356\n",
      "21/12/19 14:57:25 INFO SparkUI: Stopped Spark web UI at http://e65d5b46bbe5:4040\n",
      "21/12/19 14:57:25 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "21/12/19 14:57:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "21/12/19 14:57:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/12/19 14:57:25 INFO MemoryStore: MemoryStore cleared\n",
      "21/12/19 14:57:25 INFO BlockManager: BlockManager stopped\n",
      "21/12/19 14:57:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/12/19 14:57:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/12/19 14:57:25 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/12/19 14:57:26 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/12/19 14:57:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-10ec90b0-af32-4c74-a380-372546869337/pyspark-92f3548f-cbff-4f90-96bb-aa517fafd214\n",
      "21/12/19 14:57:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-538fc7f7-3b0d-44c9-aa15-9b15fad009ae\n",
      "21/12/19 14:57:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-10ec90b0-af32-4c74-a380-372546869337\n"
     ]
    }
   ],
   "source": [
    "!/opt/spark/bin/spark-submit \\\n",
    "  --master spark://master:7077 \\\n",
    "  /home/data/0.test-pi.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
