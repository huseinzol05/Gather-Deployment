{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646ccb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 19:27:04.819592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-22 19:27:04.819648: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflowonspark import TFCluster\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from tensorflowonspark import TFCluster\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a17d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_fun(args, ctx):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflowonspark import compat, TFNode\n",
    "\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # single node\n",
    "    # single_worker_model = build_and_compile_cnn_model()\n",
    "    # single_worker_model.fit(x=train_datasets, epochs=3)\n",
    "\n",
    "    tf_feed = TFNode.DataFeed(ctx.mgr, False)\n",
    "\n",
    "    def rdd_generator():\n",
    "        while not tf_feed.should_stop():\n",
    "            batch = tf_feed.next_batch(1)\n",
    "            if len(batch) > 0:\n",
    "                example = batch[0]\n",
    "                image = np.array(example[0]).astype(np.float32) / 255.0\n",
    "                image = np.reshape(image, (28, 28, 1))\n",
    "                label = np.array(example[1]).astype(np.float32)\n",
    "                label = np.reshape(label, (1,))\n",
    "                yield (image, label)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(rdd_generator, (tf.float32, tf.float32),\n",
    "                                        (tf.TensorShape([28, 28, 1]), tf.TensorShape([1])))\n",
    "    ds = ds.batch(args['batch_size'])\n",
    "\n",
    "    # this fails\n",
    "    # callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=args.model_dir)]\n",
    "    tf.io.gfile.makedirs(args['model_dir'])\n",
    "    filepath = args['model_dir'] + \"/weights-{epoch:04d}\"\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath, verbose=1, save_weights_only=True)]\n",
    "\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_and_compile_cnn_model()\n",
    "\n",
    "    # Note: MultiWorkerMirroredStrategy (CollectiveAllReduceStrategy) is synchronous,\n",
    "    # so we need to ensure that all workers complete training before any of them run out of data from the RDD.\n",
    "    # And given that Spark RDD partitions (and partition sizes) can be non-evenly divisible by num_workers,\n",
    "    # we'll just stop training at 90% of the total expected number of steps.\n",
    "    steps_per_epoch = 60000 / args['batch_size']\n",
    "    steps_per_epoch_per_worker = steps_per_epoch / ctx.num_workers\n",
    "    max_steps_per_worker = steps_per_epoch_per_worker * 0.9\n",
    "\n",
    "    multi_worker_model.fit(x=ds, epochs=args['epochs'], steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\n",
    "\n",
    "    compat.export_saved_model(multi_worker_model, args['export_dir'], ctx.job_name == 'chief')\n",
    "\n",
    "    # terminating feed tells spark to skip processing further partitions\n",
    "    tf_feed.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd2ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/22 19:27:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=SparkConf().setMaster('spark://master:7077').setAppName(\"mnist_keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9730af3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7b8be254d878:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mnist_keras</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://master:7077 appName=mnist_keras>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69314d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "executors = sc._conf.get(\"spark.executor.instances\")\n",
    "num_executors = int(executors) if executors is not None else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfbdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size': 16,\n",
    "    'cluster_size': num_executors,\n",
    "    'epochs': 3,\n",
    "    'images_labels': 'hdfs://hdfs:9000/user/mnist/csv/train',\n",
    "    'model_dir': 'hdfs://hdfs:9000/user/mnist/model',\n",
    "    'export_dir': 'hdfs://hdfs:9000/user/mnist/export'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f357cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(ln):\n",
    "    vec = [int(x) for x in ln.split(',')]\n",
    "    return (vec[1:], vec[0])\n",
    "\n",
    "images_labels = sc.textFile(args['images_labels']).map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f654d83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 19:27:26,687 INFO (MainThread-75) Reserving TFSparkNodes \n",
      "2022-02-22 19:27:26,691 INFO (MainThread-75) cluster_template: {'chief': [0]}\n",
      "2022-02-22 19:27:26,696 INFO (MainThread-75) Reservation server binding to port 0\n",
      "2022-02-22 19:27:26,698 INFO (MainThread-75) listening for reservations at ('172.20.0.5', 40053)\n",
      "2022-02-22 19:27:26,706 INFO (MainThread-75) Starting TensorFlow on executors\n",
      "2022-02-22 19:27:26,735 INFO (MainThread-75) Waiting for TFSparkNodes to start\n",
      "2022-02-22 19:27:26,739 INFO (MainThread-75) waiting for 1 reservations\n",
      "2022-02-22 19:27:27,740 INFO (MainThread-75) waiting for 1 reservations+ 1) / 1]\n",
      "2022-02-22 19:27:28,745 INFO (MainThread-75) waiting for 1 reservations\n",
      "2022-02-22 19:27:29,748 INFO (MainThread-75) waiting for 1 reservations\n",
      "2022-02-22 19:27:30,751 INFO (MainThread-75) waiting for 1 reservations\n",
      "2022-02-22 19:27:31,756 INFO (MainThread-75) all reservations completed\n",
      "2022-02-22 19:27:31,758 INFO (MainThread-75) All TFSparkNodes started\n",
      "2022-02-22 19:27:31,759 INFO (MainThread-75) {'executor_id': 0, 'host': '172.20.0.6', 'job_name': 'chief', 'task_index': 0, 'port': 40291, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-59zst9pb/listener-ftzwacgw', 'authkey': b'AA\\xcb\\x17RVJ\\x03\\x95\"\\x85\\x08\\xd4\\x13\\x1a\\xca'}\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cluster = TFCluster.run(sc, \n",
    "                        main_fun, \n",
    "                        args, \n",
    "                        args['cluster_size'],\n",
    "                        num_ps=0,\n",
    "                        input_mode=TFCluster.InputMode.SPARK,\n",
    "                        master_node='chief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2447c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 19:27:37,330 INFO (MainThread-75) Feeding training data\n",
      "[Stage 1:>                                                         (0 + 2) / 30]\r"
     ]
    }
   ],
   "source": [
    "cluster.train(images_labels, args['epochs'])\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
